---
layout: post
title: "【Kaggle日記】 2019/04/09"
categories:
  - Kaggle
tags:
  - ML
---

# 【Kaggle日記】 2019/04/09

文系大学3年生が  
2019年度内までにKaggle Masterになる  
過程において、
その日学んだことをアウトプットしていきます。
間違いやご指摘などが御座いましたらご教示願います！

## バックグラウンド

春から文系の大学3年生です。
プログラミング・機械学習経験ゼロの状態から去年の夏頃fast.aiをきっかけに機械学習に触れ始めました。
ここ3ヶ月ほど、Kaggleに挑戦しようと試みたのですが、毎度のことhigh-scoring-kernelを模写して終わることが続きました。。。
そもそも、kernel模写してても分からないことだらけで手も足も出ず。。。
このままではマズイと思い、
一念発起して【Kaggle日記】始めます！

## モデル別得意領域

### 線形モデル

ex) Logistic Regression, SVM

それぞれの損失関数を用いて、線形の区域分けにより、グループ別に分類する

得意領域 高次元の過疎(sparse)なデータ
不得意領域 比較的簡易的な手法であるため、限界が存在する

### 樹木モデル

ex) GBDT, Random Forest

「木構造の条件分岐で分類・予測(回帰)を行う」

得意領域 精度が高く、目的変数が多くても作動する
不得意領域 条件分岐のため、線形的なの特徴を見い出すのが不正確な場合がある

### kNNモデル

ex) k-Nearest-Neighbors

近い距離ごとにグルーブ別に分類する

得意領域 シンプルな手法であるにも関わらず有用な情報が包容されている事が多い
不得意領域 意味的な(semantic)情報を抽出することができない

### ニューラルネットワークモデル

ex) 深層学習

人間のニューロンを模倣した数理モデル

得意領域 「情報空間内における予測や識別」
不得意領域 未知の情報に対する予測や識別

## 参考・引用文献

- [Coursera How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science)
- [「樹木モデルとランダムフォレスト－機械学習による分類・予測－」－データマイニングセミナー様のスライド](https://www.slideshare.net/hamadakoichi/tree-basedmodelsandrandomforests)
- [Random Forests explained intuitively](https://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively)
- [Gradient Boosting explained [demonstration]](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)
- [Introduction to k-Nearest Neighbors: Simplified (with implementation in Python)](https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/)
- [ディープラーニングの得意なこと、不得意なこと](https://cpoint-lab.co.jp/article/201804/2133/)

## 最後に

改めてこうしてMLの手法を学び直してみると再発見が多かったです。

1. スクラッチから手法を書けるようにする
2. LightGBMやXGBoost のソースコードのぞいてみる
3. Highest Scoring Kernel を何としてでも一度超える。。。

最後に  
間違いやご指摘などが御座いましたらご教示願います！